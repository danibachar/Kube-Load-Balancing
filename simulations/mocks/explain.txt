
Testing container environments

Latency Emulation tool for docker - Pumba
https://github.com/alexei-led/pumba

Pumba is a chaos testing command line tool for Docker containers.
Pumba disturbs your containers by crashing containerized application,
emulating network failures and stress-testing container resources (cpu, memory, fs, io, and others).
Supports - {<empty> | uniform | normal | pareto |  paretonormal}


############################
# Testing Plan and Metrics #
############################

# Optimization Objective Variables
1) Latency:

2) Pricing:

3) More:

# Environment Variables

1) Load - Testing load on web based systems is usually measured in users (e.g a demand of x users for t time).
A system is configured to hold X amount of users/RPS at normal time and usually have an extra resources to hold more.
We determine the amount of RPS that an average user generates, and deduce how our system will handle a certain amount of users.

Load does not change drastically in a matter of minutes and is usually predictable per given time frames (unless under attack).
To measure different load situations, we will need to generate load for different time frames.

As our system will probably update once every 5-20 minutes, chagenes in load within this time frames
are not relevant to our base case.

A system is usually expected to handle a certain load - number of users/ or RPS.
According to literature [] there are several known cases for load on a system.
So we basically need to generate load from several known cases:
1) low volume - low number of users
2) benchmark volume - average (regular) number of users
3) load test - maximum number of users
4) test destructively - simulate attack - much over then the maximal number of users the system suppose to handle

Under each condition we shall test the system with different load parameters

1) ramp up time: [citation]
- In some use cases you might want users to join at a constant pace,
  but in different use cases a sudden spike after few peaceful minutes might be what you are looking for.
2) continues functions: [citation]
- bursts - bursts of users requests with a relative short period of time
- continues
- peaks
- any combination of the above
3) User Location Distribution - should be from normal distribution [citation]


From the above explanation we can deduce that we will load is a known number.
And we can add some randomness to it by choosing values from a given set or range at random,
according to normal/uniform/pareto distribution [citation]

regular load (i.e regular tests) - that the system can handle - i.e choose (load/users/rps) at random (normal/uniform/pareto distribution) form a given range (min-max)
2) high load (i.e stress tests) - where a percentage OVER the regular load is choose at random (normal/uniform/pareto distribution) form a given range (min-max)
3) burst load (part of stress tests) - where load on the system is arriaving in burst, and between bursts it is either regular or lower than usuall
4) under attack (i.e extream stress tests) - where the system

Users simulation can use Load Testing tool such as lucust(https://locust.io/) / Jmeter, where one just configure the desire user behaviour to test...
ramp up time - total time it takes for all users to join and join function.
In some use cases you might want users to join at a constant pace, but in different use cases a sudden spike after few peaceful minutes might be what you are looking for.

Citations, examples and sources:
1) https://www.testdevlab.com/blog/2018/04/web-service-load-testing-with-simulated-users/
2) https://stackoverflow.com/questions/25739846/loadtesting-in-burst-mode-in-jmeter
3) https://jmeter-plugins.org/wiki/UltimateThreadGroup/#Ultimate-Thread-Group
4) https://www.usenix.org/legacy/publications/library/proceedings/usenix02/full_papers/adyabahl/adyabahl_html/node21.html
#####


From - http://kth.diva-portal.org/smash/get/diva2:1334280/FULLTEXT01.pdf
load status, average response(turnaround) time, and percentile of
response time

Turnaround time(TAT) is the total time taken between the submission of
a request and the return of the complete output to the customer[38].
In our
experiment, it is the time between we send a request for the product page and
the complete loading of the page. Both average TAT and percentile TAT are
used in the following evaluation. From Average TAT, we can observe the performance that represents the majority of the requests. However, the Average
TAT could be bias and not sufficient to measure the performance in some circumstances.


From - https://ieeexplore-ieee-org.ezprimo1.idc.ac.il/stamp/stamp.jsp?tp=&arnumber=7996729
However, the average turnaround time alone is not sufficient to measure
the performance, especially in the context of the time sensitive
services. Most of the time, if the user demands are not satisfied
within the given time constraints, it is as bad as service denied.
Hence, we find out the percentage of user demands which got
satisfied in the given time constraints

Also, the number of user requests each micro-service instance
can handle at average load is selected from a range of 20 to
100 requests/sec

Time needed for execution of each service is
chosen from the range of 10 to 100 milliseconds (ms) [https://arxiv.org/pdf/1506.01509.pdf]
. All
the values are selected randomly from the given ranges. Also,
we assign each user request with some delays and cost
constraints it may tolerate. We also make sure these constraints
are satisfied while scheduling the micro-services on the clouds.
